{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### metrics.py\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from preprocessing import LabeledAlignment\n",
    "\n",
    "\n",
    "def compute_precision(reference: List[LabeledAlignment], predicted: List[List[Tuple[int, int]]]) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Computes the numerator and the denominator of the precision for predicted alignments.\n",
    "    Numerator : |predicted and possible|\n",
    "    Denominator: |predicted|\n",
    "    Note that for correct metric values `sure` needs to be a subset of `possible`, but it is not the case for input data.\n",
    "    Args:\n",
    "        reference: list of alignments with fields `possible` and `sure`\n",
    "        predicted: list of alignments, i.e. lists of tuples (source_pos, target_pos)\n",
    "    Returns:\n",
    "        intersection: number of alignments that are both in predicted and possible sets, summed over all sentences\n",
    "        total_predicted: total number of predicted alignments over all sentences\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_recall(reference: List[LabeledAlignment], predicted: List[List[Tuple[int, int]]]) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Computes the numerator and the denominator of the recall for predicted alignments.\n",
    "    Numerator : |predicted and sure|\n",
    "    Denominator: |sure|\n",
    "    Args:\n",
    "        reference: list of alignments with fields `possible` and `sure`\n",
    "        predicted: list of alignments, i.e. lists of tuples (source_pos, target_pos)\n",
    "    Returns:\n",
    "        intersection: number of alignments that are both in predicted and sure sets, summed over all sentences\n",
    "        total_predicted: total number of sure alignments over all sentences\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_aer(reference: List[LabeledAlignment], predicted: List[List[Tuple[int, int]]]) -> float:\n",
    "    \"\"\"\n",
    "    Computes the alignment error rate for predictions.\n",
    "    AER=1-(|predicted and possible|+|predicted and sure|)/(|predicted|+|sure|)\n",
    "    Please use compute_precision and compute_recall to reduce code duplication.\n",
    "    Args:\n",
    "        reference: list of alignments with fields `possible` and `sure`\n",
    "        predicted: list of alignments, i.e. lists of tuples (source_pos, target_pos)\n",
    "    Returns:\n",
    "        aer: the alignment error rate\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### models.py\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from itertools import product\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing import TokenizedSentencePair\n",
    "\n",
    "\n",
    "class BaseAligner(ABC):\n",
    "    \"\"\"\n",
    "    Describes a public interface for word alignment models.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, parallel_corpus: List[TokenizedSentencePair]):\n",
    "        \"\"\"\n",
    "        Estimate alignment model parameters from a collection of parallel sentences.\n",
    "        Args:\n",
    "            parallel_corpus: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def align(self, sentences: List[TokenizedSentencePair]) -> List[List[Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Given a list of tokenized sentences, predict alignments of source and target words.\n",
    "        Args:\n",
    "            sentences: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "        Returns:\n",
    "            alignments: list of alignments for each sentence pair, i.e. lists of tuples (source_pos, target_pos).\n",
    "            Alignment positions in sentences start from 1.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DiceAligner(BaseAligner):\n",
    "    def __init__(self, num_source_words: int, num_target_words: int, threshold=0.5):\n",
    "        self.cooc = np.zeros((num_source_words, num_target_words), dtype=np.uint32)\n",
    "        self.dice_scores = None\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, parallel_corpus):\n",
    "        for sentence in parallel_corpus:\n",
    "            # use np.unique, because for a pair of words we add 1 only once for each sentence\n",
    "            for source_token in np.unique(sentence.source_tokens):\n",
    "                for target_token in np.unique(sentence.target_tokens):\n",
    "                    self.cooc[source_token, target_token] += 1\n",
    "        self.dice_scores = (2 * self.cooc.astype(np.float32) /\n",
    "                            (self.cooc.sum(0, keepdims=True) + self.cooc.sum(1, keepdims=True)))\n",
    "\n",
    "    def align(self, sentences):\n",
    "        result = []\n",
    "        for sentence in sentences:\n",
    "            alignment = []\n",
    "            for (i, source_token), (j, target_token) in product(\n",
    "                    enumerate(sentence.source_tokens, 1),\n",
    "                    enumerate(sentence.target_tokens, 1)):\n",
    "                if self.dice_scores[source_token, target_token] > self.threshold:\n",
    "                    alignment.append((i, j))\n",
    "            result.append(alignment)\n",
    "        return result\n",
    "\n",
    "\n",
    "class WordAligner(BaseAligner):\n",
    "    def __init__(self, num_source_words, num_target_words, num_iters):\n",
    "        self.num_source_words = num_source_words\n",
    "        self.num_target_words = num_target_words\n",
    "        self.translation_probs = np.full((num_source_words, num_target_words), 1 / num_target_words, dtype=np.float32)\n",
    "        self.num_iters = num_iters\n",
    "\n",
    "    def _e_step(self, parallel_corpus: List[TokenizedSentencePair]) -> List[np.array]:\n",
    "        \"\"\"\n",
    "        Given a parallel corpus and current model parameters, get a posterior distribution over alignments for each\n",
    "        sentence pair.\n",
    "        Args:\n",
    "            parallel_corpus: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "        Returns:\n",
    "            posteriors: list of np.arrays with shape (src_len, target_len). posteriors[i][j][k] gives a posterior\n",
    "            probability of target token k to be aligned to source token j in a sentence i.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _compute_elbo(self, parallel_corpus: List[TokenizedSentencePair], posteriors: List[np.array]) -> float:\n",
    "        \"\"\"\n",
    "        Compute evidence (incomplete likelihood) lower bound for a model given data and the posterior distribution\n",
    "        over latent variables.\n",
    "        Args:\n",
    "            parallel_corpus: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "            posteriors: posterior alignment probabilities for parallel sentence pairs (see WordAligner._e_step).\n",
    "        Returns:\n",
    "            elbo: the value of evidence lower bound\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _m_step(self, parallel_corpus: List[TokenizedSentencePair], posteriors: List[np.array]):\n",
    "        \"\"\"\n",
    "        Update model parameters from a parallel corpus and posterior alignment distribution. Also, compute and return\n",
    "        evidence lower bound after updating the parameters for logging purposes.\n",
    "        Args:\n",
    "            parallel_corpus: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "            posteriors: posterior alignment probabilities for parallel sentence pairs (see WordAligner._e_step).\n",
    "        Returns:\n",
    "            elbo:  the value of evidence lower bound after applying parameter updates\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, parallel_corpus):\n",
    "        \"\"\"\n",
    "        Same as in the base class, but keep track of ELBO values to make sure that they are non-decreasing.\n",
    "        Sorry for not sticking to my own interface ;)\n",
    "        Args:\n",
    "            parallel_corpus: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "        Returns:\n",
    "            history: values of ELBO after each EM-step\n",
    "        \"\"\"\n",
    "        history = []\n",
    "        for i in range(self.num_iters):\n",
    "            posteriors = self._e_step(parallel_corpus)\n",
    "            elbo = self._m_step(parallel_corpus, posteriors)\n",
    "            history.append(elbo)\n",
    "        return history\n",
    "\n",
    "    def align(self, sentences):\n",
    "        pass\n",
    "\n",
    "\n",
    "class WordPositionAligner(WordAligner):\n",
    "    def __init__(self, num_source_words, num_target_words, num_iters):\n",
    "        super().__init__(num_source_words, num_target_words, num_iters)\n",
    "        self.alignment_probs = {}\n",
    "\n",
    "    def _get_probs_for_lengths(self, src_length: int, tgt_length: int):\n",
    "        \"\"\"\n",
    "        Given lengths of a source sentence and its translation, return the parameters of a \"prior\" distribution over\n",
    "        alignment positions for these lengths. If these parameters are not initialized yet, first initialize\n",
    "        them with a uniform distribution.\n",
    "        Args:\n",
    "            src_length: length of a source sentence\n",
    "            tgt_length: length of a target sentence\n",
    "        Returns:\n",
    "            probs_for_lengths: np.array with shape (src_length, tgt_length)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _e_step(self, parallel_corpus):\n",
    "        pass\n",
    "\n",
    "    def _compute_elbo(self, parallel_corpus, posteriors):\n",
    "        pass\n",
    "\n",
    "    def _m_step(self, parallel_corpus, posteriors):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocessing.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SentencePair:\n",
    "    \"\"\"\n",
    "    Contains lists of tokens (strings) for source and target sentence\n",
    "    \"\"\"\n",
    "    source: List[str]\n",
    "    target: List[str]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TokenizedSentencePair:\n",
    "    \"\"\"\n",
    "    Contains arrays of token vocabulary indices (preferably np.int32) for source and target sentence\n",
    "    \"\"\"\n",
    "    source_tokens: np.ndarray\n",
    "    target_tokens: np.ndarray\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LabeledAlignment:\n",
    "    \"\"\"\n",
    "    Contains arrays of alignments (lists of tuples (source_pos, target_pos)) for a given sentence.\n",
    "    Positions are numbered from 1.\n",
    "    \"\"\"\n",
    "    sure: List[Tuple[int, int]]\n",
    "    possible: List[Tuple[int, int]]\n",
    "\n",
    "\n",
    "def extract_sentences(filename: str) -> Tuple[List[SentencePair], List[LabeledAlignment]]:\n",
    "    \"\"\"\n",
    "    Given a file with tokenized parallel sentences and alignments in XML format, return a list of sentence pairs\n",
    "    and alignments for each sentence.\n",
    "    Args:\n",
    "        filename: Name of the file containing XML markup for labeled alignments\n",
    "    Returns:\n",
    "        sentence_pairs: list of `SentencePair`s for each sentence in the file\n",
    "        alignments: list of `LabeledAlignment`s corresponding to these sentences\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "\n",
    "def get_token_to_index(sentence_pairs: List[SentencePair], freq_cutoff=None) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Given a parallel corpus, create two dictionaries token->index for source and target language.\n",
    "    Args:\n",
    "        sentence_pairs: list of `SentencePair`s for token frequency estimation\n",
    "        freq_cutoff: if not None, keep only freq_cutoff most frequent tokens in each language\n",
    "    Returns:\n",
    "        source_dict: mapping of token to a unique number (from 0 to vocabulary size) for source language\n",
    "        target_dict: mapping of token to a unique number (from 0 to vocabulary size) target language\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def tokenize_sents(sentence_pairs: List[SentencePair], source_dict, target_dict) -> List[TokenizedSentencePair]:\n",
    "    \"\"\"\n",
    "    Given a parallel corpus and token_to_index for each language, transform each pair of sentences from lists\n",
    "    of strings to arrays of integers. If either source or target sentence has no tokens that occur in corresponding\n",
    "    token_to_index, do not include this pair in the result.\n",
    "    \n",
    "    Args:\n",
    "        sentence_pairs: list of `SentencePair`s for transformation\n",
    "        source_dict: mapping of token to a unique number for source language\n",
    "        target_dict: mapping of token to a unique number for target language\n",
    "    Returns:\n",
    "        tokenized_sentence_pairs: sentences from sentence_pairs, tokenized using source_dict and target_dict\n",
    "    \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
