{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "s0_default: float = 1\n",
    "p_default: float = 0.2\n",
    "\n",
    "batch_size_default: int = 1\n",
    "\n",
    "alpha_default: float = 0.1\n",
    "eps_default: float = 1e-8\n",
    "\n",
    "mu_default = 1e-2\n",
    "\n",
    "tolerance_default: float = 1e-3\n",
    "max_iter_default: int = 1000\n",
    "\n",
    "class BaseDescent:\n",
    "    \"\"\"\n",
    "    A base class and examples for all functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def step(self, X: np.ndarray, y: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Descent step\n",
    "        :param iteration: iteration number\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: difference between weights\n",
    "        \"\"\"\n",
    "        return self.update_weights(self.calc_gradient(X, y), iteration)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for update_weights function\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for calc_gradient function\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Full gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        new_weigths = self.w - self.eta(iteration) * gradient\n",
    "\n",
    "        return new_weigths\n",
    "        # TODO: implement updating weights function\n",
    "#         raise NotImplementedError('GradientDescent update_weights function not implemented')\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        \n",
    "        diff = X @ self.w - y\n",
    "        \n",
    "        grad = 2 * X.transpose() @ diff\n",
    "        \n",
    "        return grad\n",
    "        # TODO: implement calculating gradient function\n",
    "#         raise NotImplementedError('GradientDescent calc_gradient function not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionCustom:\n",
    "    \"\"\"\n",
    "    Linear regression class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, descent, lambda_, tolerance: float = tolerance_default, max_iter: int = max_iter_default):\n",
    "        \"\"\"\n",
    "        :param descent: Descent class\n",
    "        :param tolerance: float stopping criterion for square of euclidean norm of weight difference\n",
    "        :param max_iter: int stopping criterion for iterations\n",
    "        \"\"\"\n",
    "        self.descent = descent\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = int(max_iter)\n",
    "        self.loss_history = []\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "        self.w = np.nan\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> LinearRegression:\n",
    "        \"\"\"\n",
    "        Getting objects, fitting descent weights\n",
    "        :param X: objects' features\n",
    "        :param y: objects' target\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        \n",
    "        for iteration in range(1, self.max_iter + 1):\n",
    "            w_old = self.w\n",
    "            w_new = self.descent(self.w, lambda_=self.lambda_).step(X, y, iteration=iteration)\n",
    "            self.w = w_new\n",
    "            \n",
    "            self.calc_loss(X, y)\n",
    "#             print(iteration, self.loss_history[-1])\n",
    "            \n",
    "            if np.linalg.norm(w_old - w_new) < self.tolerance: \n",
    "                break\n",
    "                        \n",
    "        return self\n",
    "        # TODO: fit weights to X and y\n",
    "#         raise NotImplementedError('LinearRegression fit function not implemented')\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, predicting targets\n",
    "        :param X: objects' features\n",
    "        :return: predicted targets\n",
    "        \"\"\"\n",
    "        prediction = X @ self.w\n",
    "        \n",
    "        return prediction\n",
    "        # TODO: calculate prediction for X\n",
    "#         raise NotImplementedError('LinearRegression predict function not implemented')\n",
    "\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating loss\n",
    "        :param X: objects' features\n",
    "        :param y: objects' target\n",
    "        \"\"\"\n",
    "        loss = ((X @ self.w - y)**2).sum()\n",
    "        \n",
    "        self.loss_history.append(loss) \n",
    "        \n",
    "        # TODO: calculate loss and save it to loss_history\n",
    "#         raise NotImplementedError('LinearRegression calc_loss function not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)\n",
    "\n",
    "lambda_ = 1e-2\n",
    "w0 = np.zeros(dimension)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>monthOfRegistration</th>\n",
       "      <th>dateCreated</th>\n",
       "      <th>lastSeen</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>golf</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>manuell</td>\n",
       "      <td>benzin</td>\n",
       "      <td>nein</td>\n",
       "      <td>75</td>\n",
       "      <td>150000</td>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-03-17 00:00:00</td>\n",
       "      <td>2016-03-17 17:40:17</td>\n",
       "      <td>91074</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skoda</td>\n",
       "      <td>fabia</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>manuell</td>\n",
       "      <td>diesel</td>\n",
       "      <td>nein</td>\n",
       "      <td>69</td>\n",
       "      <td>90000</td>\n",
       "      <td>2008</td>\n",
       "      <td>7</td>\n",
       "      <td>2016-03-31 00:00:00</td>\n",
       "      <td>2016-04-06 10:17:21</td>\n",
       "      <td>60437</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bmw</td>\n",
       "      <td>3er</td>\n",
       "      <td>limousine</td>\n",
       "      <td>manuell</td>\n",
       "      <td>benzin</td>\n",
       "      <td>ja</td>\n",
       "      <td>102</td>\n",
       "      <td>150000</td>\n",
       "      <td>1995</td>\n",
       "      <td>10</td>\n",
       "      <td>2016-04-04 00:00:00</td>\n",
       "      <td>2016-04-06 19:17:07</td>\n",
       "      <td>33775</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peugeot</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>cabrio</td>\n",
       "      <td>manuell</td>\n",
       "      <td>benzin</td>\n",
       "      <td>nein</td>\n",
       "      <td>109</td>\n",
       "      <td>150000</td>\n",
       "      <td>2004</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-04-01 00:00:00</td>\n",
       "      <td>2016-04-05 18:18:39</td>\n",
       "      <td>67112</td>\n",
       "      <td>2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mazda</td>\n",
       "      <td>3_reihe</td>\n",
       "      <td>limousine</td>\n",
       "      <td>manuell</td>\n",
       "      <td>benzin</td>\n",
       "      <td>nein</td>\n",
       "      <td>105</td>\n",
       "      <td>150000</td>\n",
       "      <td>2004</td>\n",
       "      <td>12</td>\n",
       "      <td>2016-03-26 00:00:00</td>\n",
       "      <td>2016-04-06 10:45:34</td>\n",
       "      <td>96224</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        brand    model vehicleType  gearbox fuelType notRepairedDamage  \\\n",
       "0  volkswagen     golf  kleinwagen  manuell   benzin              nein   \n",
       "1       skoda    fabia  kleinwagen  manuell   diesel              nein   \n",
       "2         bmw      3er   limousine  manuell   benzin                ja   \n",
       "3     peugeot  2_reihe      cabrio  manuell   benzin              nein   \n",
       "4       mazda  3_reihe   limousine  manuell   benzin              nein   \n",
       "\n",
       "   powerPS  kilometer  yearOfRegistration  monthOfRegistration  \\\n",
       "0       75     150000                2001                    6   \n",
       "1       69      90000                2008                    7   \n",
       "2      102     150000                1995                   10   \n",
       "3      109     150000                2004                    8   \n",
       "4      105     150000                2004                   12   \n",
       "\n",
       "           dateCreated             lastSeen  postalCode  price  \n",
       "0  2016-03-17 00:00:00  2016-03-17 17:40:17       91074   1500  \n",
       "1  2016-03-31 00:00:00  2016-04-06 10:17:21       60437   3600  \n",
       "2  2016-04-04 00:00:00  2016-04-06 19:17:07       33775    650  \n",
       "3  2016-04-01 00:00:00  2016-04-05 18:18:39       67112   2200  \n",
       "4  2016-03-26 00:00:00  2016-04-06 10:45:34       96224   2000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'data/autos.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['monthOfRegistration', 'dateCreated', 'lastSeen', \\\n",
    "                       'postalCode', 'price'], inplace=False)\n",
    "y = data['price']\n",
    "\n",
    "numerical = ['powerPS', 'kilometer', 'yearOfRegistration']\n",
    "categorical = ['brand', 'model', 'vehicleType', 'gearbox', 'fuelType',\n",
    "       'notRepairedDamage']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "         ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
    "         ('num', StandardScaler(), numerical)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8395920.392735025\n",
      "                pred  y_test\n",
      "117337   9449.666667   11500\n",
      "192600  11543.600000   12800\n",
      "209225   1967.166667    1650\n",
      "30818    7907.636364    7890\n",
      "220792   7719.454545    9989\n",
      "85576    1304.104167    1050\n",
      "122929   1864.000000    1900\n",
      "256      3635.714286    3600\n",
      "80889   17371.142857    9600\n",
      "77520    5671.800000    5450\n",
      "217088   4694.181818    4545\n",
      "128403   3456.666667    3350\n",
      "211817   3916.500000    3900\n",
      "148783   5799.000000    5200\n",
      "183904   3703.250000    3900\n",
      "91101    2481.125000    1600\n",
      "55771    9031.666667    9395\n",
      "146394  18297.000000   21000\n",
      "5298     2468.900000    5600\n",
      "1499     1606.990654    1800\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('ohe_and_scaling', column_transformer),\n",
    "    ('estimator', DecisionTreeRegressor(max_depth=100, min_samples_leaf=6))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "\n",
    "print(mean_squared_error(pred, y_test))\n",
    "\n",
    "print(pd.DataFrame({'pred':pred[:20], 'y_test':y_test[:20]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 25207755977731.07\n",
      "2 48306207093171.22\n",
      "3 105444396326566.23\n",
      "4 231564950667141.06\n",
      "5 494177907864486.8\n",
      "6 1016056090567333.0\n",
      "7 2009683118526653.0\n",
      "8 3826218413558726.5\n",
      "9 7020606020133512.0\n",
      "10 1.2431977788103058e+16\n",
      "11 2.1274534980311044e+16\n",
      "12 3.5228791282309144e+16\n",
      "13 5.6517064546828184e+16\n",
      "14 8.794159349077346e+16\n",
      "15 1.3286024925801632e+17\n",
      "16 1.950752892932076e+17\n",
      "17 2.7861645305943978e+17\n",
      "18 3.8741160280441376e+17\n",
      "19 5.248554700054857e+17\n",
      "20 6.933077738296472e+17\n",
      "21 8.935725483990787e+17\n",
      "22 1.1244266314914193e+18\n",
      "23 1.3822726600406141e+18\n",
      "24 1.6609874139704963e+18\n",
      "25 1.9520188074656369e+18\n",
      "26 2.244755692515871e+18\n",
      "27 2.5271579328764114e+18\n",
      "28 2.786595878237355e+18\n",
      "29 3.010815151325278e+18\n",
      "30 3.1889206741840717e+18\n",
      "31 3.3122673035683215e+18\n",
      "32 3.375154701117193e+18\n",
      "33 3.375249419406283e+18\n",
      "34 3.313693231579689e+18\n",
      "35 3.194897321742972e+18\n",
      "36 3.026060467419762e+18\n",
      "37 2.816479975055079e+18\n",
      "38 2.5767428835311964e+18\n",
      "39 2.3178902031163356e+18\n",
      "40 2.050639471143972e+18\n",
      "41 1.7847333933133627e+18\n",
      "42 1.5284587158247764e+18\n",
      "43 1.2883540003302413e+18\n",
      "44 1.0691014826658405e+18\n",
      "45 8.735795298715689e+17\n",
      "46 7.030399454407724e+17\n",
      "47 5.573688253220424e+17\n",
      "48 4.3539011145970534e+17\n",
      "49 3.3517600875163974e+17\n",
      "50 2.5433630033466445e+17\n",
      "51 1.902676219469001e+17\n",
      "52 1.4035252443190498e+17\n",
      "53 1.0210566441439267e+17\n",
      "54 7.327016315320842e+16\n",
      "55 5.187090717498345e+16\n",
      "56 3.623347847003891e+16\n",
      "57 2.4977832856518704e+16\n",
      "58 1.69952113082806e+16\n",
      "59 1.1415487441502308e+16\n",
      "60 7570583701507908.0\n",
      "61 4958001111881276.0\n",
      "62 3207082001380726.5\n",
      "63 2049457818533043.8\n",
      "64 1294255367649085.5\n",
      "65 808029839931729.5\n",
      "66 499021011269053.25\n",
      "67 305138171084788.06\n",
      "68 185017804204223.28\n",
      "69 111520143079732.64\n",
      "70 67100119139419.625\n",
      "71 40578490280982.24\n",
      "72 24932399442063.9\n",
      "73 15810890310352.314\n",
      "74 10554823603137.643\n",
      "75 7560618324028.61\n",
      "76 5873864328153.102\n",
      "77 4933813125583.464\n",
      "78 4415168438728.698\n",
      "79 4131570567914.039\n",
      "80 3977564697689.979\n",
      "81 3894198901336.553\n",
      "82 3848910579380.81\n",
      "83 3823925158105.611\n",
      "84 3809652912346.1265\n",
      "85 3800979292577.5605\n",
      "86 3795207351149.1157\n",
      "87 3790930787141.028\n",
      "88 3787424300197.303\n",
      "89 3784317789426.5215\n",
      "90 3781424321245.52\n",
      "91 3778650360882.578\n",
      "92 3775949473893.34\n",
      "93 3773298721034.2515\n",
      "94 3770686759313.7646\n",
      "95 3768107910849.6597\n",
      "96 3765559238976.618\n",
      "97 3763039122480.269\n",
      "98 3760546567765.061\n",
      "99 3758080880473.918\n",
      "100 3755641510281.5386\n",
      "101 3753227978216.787\n",
      "102 3750839842897.7065\n",
      "103 3748476684922.721\n",
      "104 3746138099647.7285\n",
      "105 3743823693799.3315\n",
      "106 3741533083827.928\n",
      "107 3739265895044.7876\n",
      "108 3737021761111.6743\n",
      "109 3734800323690.108\n",
      "110 3732601232164.812\n",
      "111 3730424143403.7134\n",
      "112 3728268721537.9814\n",
      "113 3726134637754.8438\n",
      "114 3724021570099.8525\n",
      "115 3721929203287.0786\n",
      "116 3719857228516.3486\n",
      "117 3717805343297.0776\n",
      "118 3715773251278.2603\n",
      "119 3713760662084.376\n",
      "120 3711767291156.9062\n",
      "121 3709792859601.223\n",
      "122 3707837094038.6333\n",
      "123 3705899726463.354\n",
      "124 3703980494104.1978\n",
      "125 3702079139290.8135\n",
      "126 3700195409324.248\n",
      "127 3698329056351.6987\n",
      "128 3696479837245.2603\n",
      "129 3694647513484.5186\n",
      "130 3692831851042.8457\n",
      "131 3691032620277.232\n",
      "132 3689249595821.551\n",
      "133 3687482556483.089\n",
      "134 3685731285142.2515\n",
      "135 3683995568655.2886\n",
      "136 3682275197759.972\n",
      "137 3680569966984.0674\n",
      "138 3678879674556.537\n",
      "139 3677204122321.3496\n",
      "140 3675543115653.8174\n",
      "141 3673896463379.3584\n",
      "142 3672263977694.6133\n",
      "143 3670645474090.804\n",
      "144 3669040771279.304\n",
      "145 3667449691119.2837\n",
      "146 3665872058547.4155\n",
      "147 3664307701509.5186\n",
      "148 3662756450894.1284\n",
      "149 3661218140467.869\n",
      "150 3659692606812.622\n",
      "151 3658179689264.3867\n",
      "152 3656679229853.8125\n",
      "153 3655191073248.322\n",
      "154 3653715066695.777\n",
      "155 3652251059969.666\n",
      "156 3650798905315.7114\n",
      "157 3649358457399.8975\n",
      "158 3647929573257.856\n",
      "159 3646512112245.561\n",
      "160 3645105935991.3\n",
      "161 3643710908348.881\n",
      "162 3642326895352.031\n",
      "163 3640953765169.9595\n",
      "164 3639591388064.039\n",
      "165 3638239636345.574\n",
      "166 3636898384334.632\n",
      "167 3635567508319.8877\n",
      "168 3634246886519.4756\n",
      "169 3632936399042.7886\n",
      "170 3631635927853.223\n",
      "171 3630345356731.8286\n",
      "172 3629064571241.832\n",
      "173 3627793458694.027\n",
      "174 3626531908112.9844\n",
      "175 3625279810204.074\n",
      "176 3624037057321.2646\n",
      "177 3622803543435.6865\n",
      "178 3621579164104.94\n",
      "179 3620363816443.1055\n",
      "180 3619157399091.468\n",
      "181 3617959812189.908\n",
      "182 3616770957348.9644\n",
      "183 3615590737622.518\n",
      "184 3614419057481.1235\n",
      "185 3613255822785.9277\n",
      "186 3612100940763.186\n",
      "187 3610954319979.3555\n",
      "188 3609815870316.737\n",
      "189 3608685502949.673\n",
      "190 3607563130321.2593\n",
      "191 3606448666120.5806\n",
      "192 3605342025260.455\n",
      "193 3604243123855.6426\n",
      "194 3603151879201.5547\n",
      "195 3602068209753.407\n",
      "196 3600992035105.8354\n",
      "197 3599923275972.9424\n",
      "198 3598861854168.7773\n",
      "199 3597807692588.2275\n",
      "200 3596760715188.318\n",
      "201 3595720846969.9136\n",
      "202 3594688013959.789\n",
      "203 3593662143193.096\n",
      "204 3592643162696.1836\n",
      "205 3591631001469.7744\n",
      "206 3590625589472.502\n",
      "207 3589626857604.773\n",
      "208 3588634737692.971\n",
      "209 3587649162473.9824\n",
      "210 3586670065580.0347\n",
      "211 3585697381523.836\n",
      "212 3584731045684.0337\n",
      "213 3583770994290.953\n",
      "214 3582817164412.613\n",
      "215 3581869493941.046\n",
      "216 3580927921578.8667\n",
      "217 3579992386826.1187\n",
      "218 3579062829967.379\n",
      "219 3578139192059.1167\n",
      "220 3577221414917.293\n",
      "221 3576309441105.2173\n",
      "222 3575403213921.6206\n",
      "223 3574502677388.9756\n",
      "224 3573607776242.037\n",
      "225 3572718455916.5967\n",
      "226 3571834662538.467\n",
      "227 3570956342912.6562\n",
      "228 3570083444512.7686\n",
      "229 3569215915470.593\n",
      "230 3568353704565.888\n",
      "231 3567496761216.369\n",
      "232 3566645035467.8745\n",
      "233 3565798477984.713\n",
      "234 3564957040040.2036\n",
      "235 3564120673507.374\n",
      "236 3563289330849.8394\n",
      "237 3562462965112.8584\n",
      "238 3561641529914.5312\n",
      "239 3560824979437.178\n",
      "240 3560013268418.865\n",
      "241 3559206352145.0874\n",
      "242 3558404186440.598\n",
      "243 3557606727661.3926\n",
      "244 3556813932686.827\n",
      "245 3556025758911.8867\n",
      "246 3555242164239.588\n",
      "247 3554463107073.5117\n",
      "248 3553688546310.4814\n",
      "249 3552918441333.353\n",
      "250 3552152752003.946\n",
      "251 3551391438656.093\n",
      "252 3550634462088.8086\n",
      "253 3549881783559.5845\n",
      "254 3549133364777.7935\n",
      "255 3548389167898.209\n",
      "256 3547649155514.6406\n",
      "257 3546913290653.675\n",
      "258 3546181536768.529\n",
      "259 3545453857732.998\n",
      "260 3544730217835.52\n",
      "261 3544010581773.3325\n",
      "262 3543294914646.726\n",
      "263 3542583181953.4033\n",
      "264 3541875349582.9277\n",
      "265 3541171383811.266\n",
      "266 3540471251295.4185\n",
      "267 3539774919068.147\n",
      "268 3539082354532.7827\n",
      "269 3538393525458.124\n",
      "270 3537708399973.416\n",
      "271 3537026946563.415\n",
      "272 3536349134063.5376\n",
      "273 3535674931655.0737\n",
      "274 3535004308860.501\n",
      "275 3534337235538.8545\n",
      "276 3533673681881.1836\n",
      "277 3533013618406.08\n",
      "278 3532357015955.2695\n",
      "279 3531703845689.295\n",
      "280 3531054079083.24\n",
      "281 3530407687922.5474\n",
      "282 3529764644298.8906\n",
      "283 3529124920606.107\n",
      "284 3528488489536.21\n",
      "285 3527855324075.4507\n",
      "286 3527225397500.4497\n",
      "287 3526598683374.385\n",
      "288 3525975155543.2397\n",
      "289 3525354788132.116\n",
      "290 3524737555541.595\n",
      "291 3524123432444.16\n",
      "292 3523512393780.67\n",
      "293 3522904414756.9004\n",
      "294 3522299470840.1216\n",
      "295 3521697537755.7383\n",
      "296 3521098591483.9736\n",
      "297 3520502608256.6196\n",
      "298 3519909564553.813\n",
      "299 3519319437100.8853\n",
      "300 3518732202865.24\n",
      "301 3518147839053.29\n",
      "302 3517566323107.438\n",
      "303 3516987632703.093\n",
      "304 3516411745745.7505\n",
      "305 3515838640368.0977\n",
      "306 3515268294927.172\n",
      "307 3514700688001.56\n",
      "308 3514135798388.6367\n",
      "309 3513573605101.844\n",
      "310 3513014087368.016\n",
      "311 3512457224624.7344\n",
      "312 3511902996517.732\n",
      "313 3511351382898.327\n",
      "314 3510802363820.8975\n",
      "315 3510255919540.392\n",
      "316 3509712030509.881\n",
      "317 3509170677378.1357\n",
      "318 3508631840987.2476\n",
      "319 3508095502370.2783\n",
      "320 3507561642748.9517\n",
      "321 3507030243531.3667\n",
      "322 3506501286309.7505\n",
      "323 3505974752858.245\n",
      "324 3505450625130.7236\n",
      "325 3504928885258.6313\n",
      "326 3504409515548.865\n",
      "327 3503892498481.6865\n",
      "328 3503377816708.648\n",
      "329 3502865453050.5654\n",
      "330 3502355390495.5146\n",
      "331 3501847612196.8447\n",
      "332 3501342101471.233\n",
      "333 3500838841796.7666\n",
      "334 3500337816811.0376\n",
      "335 3499839010309.2812\n",
      "336 3499342406242.5234\n",
      "337 3498847988715.7695\n",
      "338 3498355741986.2095\n",
      "339 3497865650461.4453\n",
      "340 3497377698697.749\n",
      "341 3496891871398.342\n",
      "342 3496408153411.698\n",
      "343 3495926529729.8696\n",
      "344 3495446985486.831\n",
      "345 3494969505956.8594\n",
      "346 3494494076552.917\n",
      "347 3494020682825.075\n",
      "348 3493549310458.943\n",
      "349 3493079945274.1313\n",
      "350 3492612573222.7207\n",
      "351 3492147180387.77\n",
      "352 3491683752981.8296\n",
      "353 3491222277345.478\n",
      "354 3490762739945.8813\n",
      "355 3490305127375.3677\n",
      "356 3489849426350.027\n",
      "357 3489395623708.3164\n",
      "358 3488943706409.7007\n",
      "359 3488493661533.3\n",
      "360 3488045476276.552\n",
      "361 3487599137953.9043\n",
      "362 3487154633995.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363 3486711951945.9634\n",
      "364 3486271079463.003\n",
      "365 3485832004316.2964\n",
      "366 3485394714386.1943\n",
      "367 3484959197662.516\n",
      "368 3484525442243.349\n",
      "369 3484093436333.8643\n",
      "370 3483663168245.152\n",
      "371 3483234626393.056\n",
      "372 3482807799297.049\n",
      "373 3482382675579.094\n",
      "374 3481959243962.5405\n",
      "375 3481537493271.026\n",
      "376 3481117412427.395\n",
      "377 3480698990452.628\n",
      "378 3480282216464.7837\n",
      "379 3479867079677.963\n",
      "380 3479453569401.2744\n",
      "381 3479041675037.8184\n",
      "382 3478631386083.69\n",
      "383 3478222692126.977\n",
      "384 3477815582846.7915\n",
      "385 3477410048012.297\n",
      "386 3477006077481.757\n",
      "387 3476603661201.5913\n",
      "388 3476202789205.446\n",
      "389 3475803451613.274\n",
      "390 3475405638630.427\n",
      "391 3475009340546.754\n",
      "392 3474614547735.726\n",
      "393 3474221250653.5493\n",
      "394 3473829439838.31\n",
      "395 3473439105909.114\n",
      "396 3473050239565.249\n",
      "397 3472662831585.3477\n",
      "398 3472276872826.564\n",
      "399 3471892354223.7656\n",
      "400 3471509266788.724\n",
      "401 3471127601609.3257\n",
      "402 3470747349848.789\n",
      "403 3470368502744.8853\n",
      "404 3469991051609.1724\n",
      "405 3469614987826.2495\n",
      "406 3469240302852.999\n",
      "407 3468866988217.852\n",
      "408 3468495035520.062\n",
      "409 3468124436428.9785\n",
      "410 3467755182683.3403\n",
      "411 3467387266090.5723\n",
      "412 3467020678526.083\n",
      "413 3466655411932.588\n",
      "414 3466291458319.4204\n",
      "415 3465928809761.868\n",
      "416 3465567458400.508\n",
      "417 3465207396440.549\n",
      "418 3464848616151.1875\n",
      "419 3464491109864.964\n",
      "420 3464134869977.133\n",
      "421 3463779888945.0376\n",
      "422 3463426159287.49\n",
      "423 3463073673584.166\n",
      "424 3462722424474.989\n",
      "425 3462372404659.548\n",
      "426 3462023606896.502\n",
      "427 3461676024002.99\n",
      "428 3461329648854.0674\n",
      "429 3460984474382.127\n",
      "430 3460640493576.3374\n",
      "431 3460297699482.0894\n",
      "432 3459956085200.4443\n",
      "433 3459615643887.59\n",
      "434 3459276368754.2964\n",
      "435 3458938253065.399\n",
      "436 3458601290139.2573\n",
      "437 3458265473347.243\n",
      "438 3457930796113.225\n",
      "439 3457597251913.063\n",
      "440 3457264834274.101\n",
      "441 3456933536774.672\n",
      "442 3456603353043.6094\n",
      "443 3456274276759.758\n",
      "444 3455946301651.4976\n",
      "445 3455619421496.2637\n",
      "446 3455293630120.0815\n",
      "447 3454968921397.1\n",
      "448 3454645289249.133\n",
      "449 3454322727645.205\n",
      "450 3454001230601.1025\n",
      "451 3453680792178.933\n",
      "452 3453361406486.679\n",
      "453 3453043067677.7676\n",
      "454 3452725769950.644\n",
      "455 3452409507548.3423\n",
      "456 3452094274758.066\n",
      "457 3451780065910.7725\n",
      "458 3451466875380.7676\n",
      "459 3451154697585.291\n",
      "460 3450843526984.1167\n",
      "461 3450533358079.159\n",
      "462 3450224185414.075\n",
      "463 3449916003573.879\n",
      "464 3449608807184.5547\n",
      "465 3449302590912.673\n",
      "466 3448997349465.024\n",
      "467 3448693077588.234\n",
      "468 3448389770068.4043\n",
      "469 3448087421730.7427\n",
      "470 3447786027439.206\n",
      "471 3447485582096.139\n",
      "472 3447186080641.927\n",
      "473 3446887518054.641\n",
      "474 3446589889349.6997\n",
      "475 3446293189579.519\n",
      "476 3445997413833.1816\n",
      "477 3445702557236.0967\n",
      "478 3445408614949.672\n",
      "479 3445115582170.9883\n",
      "480 3444823454132.4697\n",
      "481 3444532226101.5674\n",
      "482 3444241893380.4453\n",
      "483 3443952451305.657\n",
      "484 3443663895247.846\n",
      "485 3443376220611.429\n",
      "486 3443089422834.2983\n",
      "487 3442803497387.5176\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-bd385760a8c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                         max_iter=1e4, lambda_=3e-6))])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-d36fd8c67084>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mw_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mw_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-606f94b11672>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, X, y, iteration)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdifference\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \"\"\"\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-29511056e0dd>\u001b[0m in \u001b[0;36mcalc_gradient\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    558\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    559\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_vector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;31m# csr_matvec or csc_matvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matvec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('ohe_and_scaling', column_transformer),\n",
    "    ('estimator', LinearRegressionCustom(GradientDescent, \n",
    "                                         tolerance=1e-6, \n",
    "                                        max_iter=1e4, lambda_=3e-6))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "\n",
    "print(pred[0:6])\n",
    "\n",
    "mean_squared_error(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.array([[1,2], [3,4], [6,9]])\n",
    "# y_train = np.array([0, 2, 3])\n",
    "\n",
    "# X_test = np.array([[1,3], [3,1]])\n",
    "# y_test = np.array([-1, 5])\n",
    "\n",
    "# # pipeline = Pipeline(steps=[\n",
    "# #     ('estimator', LinearRegressionCustom(GradientDescent, \n",
    "# #                                          tolerance=1e-3, \n",
    "# #                                         max_iter=1e3))])\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('estimator', LinearRegression())])\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# pred = pipeline.predict(X_train)\n",
    "# print(pred, mean_squared_error(pred, y_train), pipeline.named_steps['estimator'].coef_)\n",
    "\n",
    "# print(pipeline.predict([[1,1]]))\n",
    "\n",
    "# pred = pipeline.predict(X_test)\n",
    "\n",
    "# print(pred, mean_squared_error(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.57793477e-04 1.99951519e+00 3.00010948e+00] [-0.99836311  4.99627791] 8.266678361076879e-06\n"
     ]
    }
   ],
   "source": [
    "X_train_smpl = np.array([[1,2], [3,4], [6,9]])\n",
    "y_train_smpl = np.array([0, 2, 3])\n",
    "\n",
    "X_test_smpl = np.array([[1,3], [3,1]])\n",
    "y_test_smpl = np.array([-1, 5])\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('estimator', LinearRegressionCustom(GradientDescent, \n",
    "                                         tolerance=1e-6, \n",
    "                                        max_iter=1e5, lambda_=7e-3))])\n",
    "\n",
    "pipeline.fit(X_train_smpl, y_train_smpl)\n",
    "pred = pipeline.predict(X_test_smpl)\n",
    "\n",
    "print(pipeline.predict(X_train_smpl), pipeline.predict(X_test_smpl), \\\n",
    "      mean_squared_error(pred, y_test_smpl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
